Attention:
- Do not edit this file in text editors like Word. Use a plain text editor only. In case of doubt, you can use Spyder as a text editor.
- Do not change the structure of this file. Just fill in your answers in the places provided (After the R#: tag).
- You can add lines in the spaces for your answers but your answers should be brief and straight to the point.

QUESTIONS:

Q1: Considering the datasets provided, explain the need to standardize the attribute values.
R1: The features may have different types and therefore different scales. 
By standardizing we bring them all to the same scale.


Q2: Explain how you calculated the parameters for standardization and how you used them in the test set.
R2: We used the sklearn.preprocessing.StandardScaler class, which we fit (calculate the parameters) exclusively on the training set and use to scale both the training set and test set.


Q3: Classification: Explain how you calculated the prior probability of an example belonging to a class (the probability before taking into account the attribute values ​​of the example) in your Naïve Bayes classifier implementation. You may include a relevant piece of your code if this helps you explain.
R3: We divided the amount of samples of that class by the total amount of samples.


Q4: Explain how your Naïve Bayes classifier predicts the class to which a test example belongs. You may include a relevant piece of your code if this helps you explain.
R4: It calculates the log probability of belonging to each class by summing the logarithms of the prior probability of the class and the conditonal probabilities given the attribute values, obtained using KernelDensity.score_samples(). After that, we pick the class with the max log probability.


Q5: Explain the effect of the bandwidth parameter on your classifier.
R5: For low bandwidth values the model's validation error is high and as we increase the bandwidth has a steep descend. As we increase it further, overfitting may start ocurring, resulting in the validation error slowly increasing. On the other hand, the training error is in a mostly constant growth.


Q6: Explain how you determined the best bandwidth parameter for your classifier. You may include a relevant piece of your code if this helps you explain.
R6: Using cross validation, we obtained the average validation error. We then picked the bandwidth parameter that resulted in the minimum validation error.


Q7: Explain how you obtained the best hypothesis for each classifier after optimizing all parameters.
R7: We obtained the best hypothesis of our Naive Bayes classifier by training a new classifier with the best bandwidth on the full training set. We obtained the best hypethesis using the GaussianNB classifier by training it on the full training set, without adjusting any parameters.


Q8: Show the best parameters, the estimate of the true error for each hypothesis you obtained (your classifier and the one provided by the library), the ranges in the expected number of errors given by the approximate normal test, the McNemar test values, and discuss what you can conclude from this.
R8: Estimate of true error for our hypothesis: 4.33%
Estimate of true error for the hypothesis using the library classifier: 9.46%
Range of expected number of errors for our hypethesis: 54 +/- 14.09
Range of expected number of errors for the hypothesis using the library classifier: 118 +/- 20.06
McNemar Test Result: 38.1635 >= 3.84
Number of errors exclusive to our hypethesis: 20
Number of errors exclusive to the hypothesis using the library classifier: 84
Conclusion: Given that the range of the approximate normal test for our hypethesis is strictly above the one for the hypothesis using the library classifier, the approximate normal test suggests our hypothesis is better. Given that the McNemar's Test result is greater than 3.84 and our hypethesis has less exclusive errors than the hypothesis using the library classifier, the McNemar's test suggests our hypothesis is better.


Q9: Regression: Explain what experiments and plots gave you good
evidence to choose a given model degree. 
R9: The validation mean squared error, since the training mean squared error is biased and prone to overfitting.


Q10: In the case of your mean squared error plot explain why one of the error
curves is always decreasing, while the other is not.
R10: As the degree increases the model will be better able to fit the training set, which will cause the training error to constantly decrease. However, this may eventually result in overfitting, which causes the model to have a greater error in the validation set, which means the validation error will eventually increase.


Q11: In the plots of the true versus predicted values, where would be
all the points when predicted by an ideal regressor? Justify.
R11: In the line of slope 1, for this would mean all the predicted values are equal to the true values.


Q12: Explain your validation procedure and comment on the true error
of your chosen model for unseen data.
R12: We used cross validation with 5 folds for picking the best degree. Given that this makes the validation error biased, we then used a separate test set, that wasn't used for neither for validation nor for training, to estimate the true error. The result of this estimate oscilates between 2000 meters and 5000 meters dependeing on how the data is initally split.
